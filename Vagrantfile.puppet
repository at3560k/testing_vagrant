# -*- mode: ruby -*-
# vi: set ft=ruby :

# Defined:
#
# - puppetmaster
#   - provisions dbase via puppet server automagically
# - mongo_ansible
#   - provisions mongo_n1 .. n4
#   (mongo cluster not fully functioning yet)
#   Log into ansible, accept ssh keys with proxied agent, and run playbook
#      - `keychain ~/.vagrantfile/insecure_public_key`
#      - `vagrant ssh mongo_ansible`
#      - `cd /vagrant/deployment && ansible all -m ping`
#      -  (accept ssh keys which I haven't added yet...)
#      - `ansible-playbook -i hosts site.yml  --sudo`
#      - Watch screen scroll fast
# - salt
#   - provisions salt_n1 .. n2
#
#
#######################################################################
#                          !!! WARNING !!!
#
#  Do not forget that vagrant ships with fixed SSH Keys, this is not
#  remotely suitable for anything going anywhere near production
#
#######################################################################
#
#
# Ongoing themes and problems
#   - key management and accepting new nodes or installing keys
#   - Need/Desire for more and faster caching
#   - Installing base tools to bootstrap other tools
#      - Probably easier solved with a few base images
#   - Some commands really like to run as root or people with root
#   - Instability of resources
#     - Sometimes servers go down and update scripts fail
#       - 10gen update server has had updates fail
#     - Ansible had an update change behavior in development
#     - Salt has had multiple releases in a month (and wants to deploy
#     via curl to shell...)
#   - Bootstrapping to bootstrap
#     - Base image needs key management and utilities
#     - Python/Pip/Puppet or something
#     - Key V&V is done manually in everything without any sort
#        of PKI/cert/signing infrastructure

Vagrant.configure("2") do |config|

  # Every Vagrant virtual environment requires a box to build off of.
  config.vm.box = "precise64"
  config.vm.box_url = "http://files.vagrantup.com/precise64.box"

  # cache buckets auto detection for vagrant-cachier
  #   verified working with vagrant-cachier (0.1.0)
  config.cache.auto_detect = true 

  # Note: host only networks mean no ports are blocked at all.
  #  10.11.12/24 is vbox hostonlyif vboxnet0  on dev host
   
  # We need to run our updater in apt first or packages fail
  config.vm.provision :shell, :path => "shell/bootstrap.sh"

  # Okay, this is a bit weird.  We add a provisioner for all VMs That points to
  # a submodule locally.  Which basically forces some writes to the hostfile so
  # puppet and newly created systems can talk to each other without
  # me building a custom nameserver...
  #
  # This will go away once we actually have hostnames or internal NS
  config.vm.provision :puppet do |puppet|
      # looks in manifests/default.pp automagically
      puppet.module_path = "modules"
      # Force a manual hostsfile out so these talk without using ipaddr
      puppet.manifest_file = "patchhosts.pp"
  end
 

  #####################################
  # Puppet server - local
  #####################################
  #
  # Insecure auto signs all requests...
  #
  config.vm.define :puppetmaster do |pp_config|
    pp_config.vm.hostname = "puppet.test.edac.unm.edu"
    pp_config.vm.network :private_network, ip: "10.11.12.100"

    # Yo dawg, I heard you like puppet so I provisioned puppet with ur puppet
    pp_config.vm.provision :puppet do |puppet|
      # Local manifest, puppet apply
      puppet.manifests_path = "manifests"
      puppet.manifest_file = "puppet.pp"
      # Module path for remote module we'll add in
      puppet.module_path = "modules"
    end
  end

  #####################################
  # dbase - via puppet master
  #####################################
  config.vm.define :dbase do |db_config|
    db_config.vm.hostname = 'dbserver.test.edac.unm.edu'
    db_config.vm.network :private_network, ip: "10.11.12.211"
    # Relay PG to localhost.  This may not work nicely if you have a mismatched client version
    db_config.vm.network :forwarded_port, guest: 5432, host: 5432

    # Remote server
    db_config.vm.provision :puppet_server do |puppet|
      #puppet.options = ["--verbose", "--debug"]
      puppet.puppet_server = "puppet.test.edac.unm.edu"
    end
  end


  #####################################
  # mongo team
  #####################################
  
  # It's not that we need puppet to do this, so much as I need a way for the
  # hosts to communicate with each other by hostname and keep this organized
  # by something not ipaddr
  # 
  # vagrant up /mongo_*/

  #####################################
  # Ansible driver -- local
  #####################################

  # named so it's part of our mongo regex...
  config.vm.define :mongo_ansible do |ans_config|
    ans_config.vm.hostname = 'ansible.test.edac.unm.edu'
    ans_config.vm.network :private_network, ip: "10.11.12.101"

    # Forward our agent to this machine so I don't have to handle key
    # distribution on the images
    #
    ans_config.ssh.forward_agent = true
    #  Note, we also install the pub key by vagrant's out of the box insecure key
    #  http://github.com/mitchellh/vagrant/raw/master/keys/vagrant.pub (rename
    #  to insecure_private_key.pub)
    #  And add this to my locally running agent (which is forwarded over to the
    #  vagrant box only for command & control purposes)

    # Okay, I want to avoid risking 'infrastructure'...
    #   And I'm not putting ansible on my desktop until I trust it not to
    #   install 1000 modules that will fetch a whole git tree...
    #
    ans_config.vm.provision :puppet do |puppet|
       # thick irony is thick
       puppet.module_path = "modules"
       puppet.manifest_file = "ansible.pp"  
    end
  end

  #####################################
  # mongo1:4 - via ansible master, with puppet help
  #####################################

  numMongo = 4
  ipAddrPrefix = "10.11.12.22" #effectively .220

  1.upto(numMongo) do |num|
    nodeName = ("mongo_n" + num.to_s).to_sym
    config.vm.define nodeName do |node|
      # Look mah, no hands.  Ansible needs nothing special on these...
      #  (although we aren't fireballing)
      node.vm.hostname = 'mongo' + num.to_s + '.mongo.edac.unm.edu'
      node.vm.network :private_network, ip: ipAddrPrefix + num.to_s
    end
  end

  #####################################
  # Salt master box -- local
  #####################################
  config.vm.define :salt_shaker do |salt_config|
    # Magic host name is magic.
    salt_config.vm.hostname = 'salt.test.edac.unm.edu'
    salt_config.vm.network :private_network, ip: "10.11.12.102"
    config.ssh.forward_agent = true
    # TODO: Remember ssh agent
    salt_config.vm.provision :puppet do |puppet|
       puppet.module_path = "modules"
       puppet.manifest_file = "salt.pp"  
    end
  end

  #####################################
  # Salt minion boxen
  #####################################
  numMinions = 2
  saltIpAddrPrefix = "10.11.12.23" # effectively .230

  1.upto(numMinions) do |num|
    nodeName = ("salt_n" + num.to_s).to_sym
    config.vm.define nodeName do |node_conf|
      node_conf.vm.hostname = 'salt-n' + num.to_s + '.test.edac.unm.edu'
      node_conf.vm.network :private_network, ip: saltIpAddrPrefix + num.to_s
      node_conf.ssh.forward_agent = true

      node_conf.vm.provision :puppet do |puppet|
         puppet.module_path = "modules"
         puppet.manifest_file = "snode.pp"  
      end
    end
  end

end

