# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|

  # Every Vagrant virtual environment requires a box to build off of.
  config.vm.box = "precise64"
  config.vm.box_url = "http://files.vagrantup.com/precise64.box"

  # cache buckets auto detection for vagrant-cachier
  #   verified working with vagrant-cachier (0.1.0)
  config.cache.auto_detect = true 

  # Note: host only networks mean no ports are blocked at all.
  #  10.11.12/24 is vbox hostonlyif vboxnet0  on dev host
   
  # We need to run our updater in apt first or packages fail
  config.vm.provision :shell, :path => "shell/bootstrap.sh"

  # Okay, this is a bit weird.  We add a provisioner for all VMs That points to
  # a submodule locally.  Which basically forces some writes to the hostfile so
  # puppet and newly created systems can talk to each other without
  # me building a custom nameserver...
  #
  # This will go away once we actually have hostnames or internal NS
  config.vm.provision :puppet do |puppet|
      # looks in manifests/default.pp automagically
      puppet.module_path = "modules"
      puppet.manifest_file = "patchhosts.pp"
  end
 

  #####################################
  # Puppet server - local
  #####################################
  config.vm.define :puppetmaster do |pp_config|
    pp_config.vm.hostname = "puppet.test.edac.unm.edu"
    pp_config.vm.network :private_network, ip: "10.11.12.100"

    # Hey dog, I heard you like puppet so I provisioned puppet with your puppet
    pp_config.vm.provision :puppet do |puppet|
      # Local manifest, puppet apply
      puppet.manifests_path = "manifests"
      puppet.manifest_file = "puppet.pp"
      # Module path for remote module we'll add in
      puppet.module_path = "modules"
    end
  end

  #####################################
  # appserver - via puppet master
  #####################################
  config.vm.define :appserver do |web_config|
    web_config.vm.hostname = 'appserver'
    web_config.vm.network :private_network, ip: "10.11.12.201"
    # Relay HTTPD to 8080
    web_config.vm.network :forwarded_port, guest: 80, host: 8080

    web_config.vm.provision :puppet do |puppet|
      puppet.manifests_path = "manifests"
      puppet.manifest_file = "webapp.pp"
    end
  end

  #####################################
  # dbase - via puppet master
  #####################################
  config.vm.define :dbase do |db_config|
    db_config.vm.hostname = 'dbserver.test.edac.unm.edu'
    db_config.vm.network :private_network, ip: "10.11.12.211"
    # Relay PG to localhost.  This may not work nicely if you have a mismatched client version
    db_config.vm.network :forwarded_port, guest: 5432, host: 5432

    # Remote server
    db_config.vm.provision :puppet_server do |puppet|
      #puppet.options = ["--verbose", "--debug"]
      puppet.puppet_server = "puppet.test.edac.unm.edu"
    end
  end


  #####################################
  # mongo team
  #####################################
  
  # It's not that we need puppet to do this, so much as I need a way for the
  # hosts to communicate with each other by hostname and keep this organized
  # by something not ipaddr
  # 
  # vagrant up /mongo_*/

  #####################################
  # ansible driver
  #####################################

  # named so it's part of our mongo regex...
  config.vm.define :mongo_ansible do |ans_config|
    ans_config.vm.hostname = 'ansible.test.edac.unm.edu'
    ans_config.vm.network :private_network, ip: "10.11.12.101"

    # Forward our agent to this machine so I don't have to handle key
    # distribution on the images
    #
    config.ssh.forward_agent = true
    #  Note, we also install the pub key by vagrant's out of the box insecure key
    #  http://github.com/mitchellh/vagrant/raw/master/keys/vagrant.pub (rename
    #  to insecure_private_key.pub)
    #  And add this to my locally running agent (which is forwarded over to the
    #  vagrant box only for command & control purposes)

    # Okay, I want to avoid risking 'infrastructure'...
    #   And I'm not putting ansible on my desktop until I trust it not to
    #   install 1000 modules that will fetch a whole git tree...
    #
    ans_config.vm.provision :puppet do |puppet|
       # thick irony is thick
       puppet.module_path = "modules"
       puppet.manifest_file = "ansible.pp"  
    end
  end

  #####################################
  # mongod core (mongod, replication, configuration) - via ansible master, with puppet help
  #####################################

  config.vm.define :mongo_m1 do |m1_config|
    m1_config.vm.hostname = 'mongo1.mongo.edac.unm.edu'
    m1_config.vm.network :private_network, ip: "10.11.12.221"

    #web_config.vm.provision :ansible do |ansible|
    #  ansible.playbook = "deployment/www.yml"
    #  ansible.inventory_file = "deployment/hosts"
  end

  config.vm.define :mongo_m2 do |m2_config|
    m2_config.vm.hostname = 'mongo2.mongo.edac.unm.edu'
    m2_config.vm.network :private_network, ip: "10.11.12.222"
  end

  config.vm.define :mongo_m3 do |m3_config|
    m3_config.vm.hostname = 'mongo3.mongo.edac.unm.edu'
    m3_config.vm.network :private_network, ip: "10.11.12.223"
  end
  
  #####################################
  # mongos server (shard) - via ansible master, with puppet help
  #####################################
  config.vm.define :mongo_s1 do |s1_config|
    s1_config.vm.hostname = 'mongos1.mongo.edac.unm.edu'
    s1_config.vm.network :private_network, ip: "10.11.12.231"
  end

  config.vm.define :mongo_s2 do |s2_config|
    s2_config.vm.hostname = 'mongos2.mongo.edac.unm.edu'
    s2_config.vm.network :private_network, ip: "10.11.12.232"
  end

end
